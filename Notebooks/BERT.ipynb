{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46970263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f5b7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>document.id</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>eth_group_to_code</th>\n",
       "      <th>is_ethicity_superior_meaning</th>\n",
       "      <th>is_ethicity_aggressor_meaning</th>\n",
       "      <th>is_ethicity_dangerous_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>885072939</td>\n",
       "      <td>грузин mia бред написать какой русский вообще ...</td>\n",
       "      <td>грузин</td>\n",
       "      <td>irrel</td>\n",
       "      <td>irrel</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>885072939</td>\n",
       "      <td>грузин mia бред написать какой русский вообще ...</td>\n",
       "      <td>грузин</td>\n",
       "      <td>irrel</td>\n",
       "      <td>irrel</td>\n",
       "      <td>irrel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>885072939</td>\n",
       "      <td>грузин mia бред написать какой русский вообще ...</td>\n",
       "      <td>грузин</td>\n",
       "      <td>irrel</td>\n",
       "      <td>irrel</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>885072939</td>\n",
       "      <td>грузин mia бред написать какой русский вообще ...</td>\n",
       "      <td>грузин</td>\n",
       "      <td>irrel</td>\n",
       "      <td>agressor</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>885072939</td>\n",
       "      <td>грузин mia бред написать какой русский вообще ...</td>\n",
       "      <td>грузин</td>\n",
       "      <td>irrel</td>\n",
       "      <td>agressor</td>\n",
       "      <td>irrel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  document.id                                          lemm_text  \\\n",
       "0           0    885072939  грузин mia бред написать какой русский вообще ...   \n",
       "1           1    885072939  грузин mia бред написать какой русский вообще ...   \n",
       "2           2    885072939  грузин mia бред написать какой русский вообще ...   \n",
       "3          10    885072939  грузин mia бред написать какой русский вообще ...   \n",
       "4          11    885072939  грузин mia бред написать какой русский вообще ...   \n",
       "\n",
       "  eth_group_to_code is_ethicity_superior_meaning  \\\n",
       "0            грузин                        irrel   \n",
       "1            грузин                        irrel   \n",
       "2            грузин                        irrel   \n",
       "3            грузин                        irrel   \n",
       "4            грузин                        irrel   \n",
       "\n",
       "  is_ethicity_aggressor_meaning is_ethicity_dangerous_meaning  \n",
       "0                         irrel                            no  \n",
       "1                         irrel                         irrel  \n",
       "2                         irrel                           yes  \n",
       "3                      agressor                            no  \n",
       "4                      agressor                         irrel  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('eth_group_lemm_text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e8786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Фиксирую сулчайность\n",
    "\n",
    "def seed_all(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "# seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e25e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Добавляю столбцы с лейблами для каждой категории\n",
    "def encode_columns(data, columns):\n",
    "    encoders = []\n",
    "    labels = []\n",
    "    for column in columns:\n",
    "        le = LabelEncoder()\n",
    "        data[f'label_{column}'] = le.fit_transform(data[column].values)\n",
    "        encoders.append(le)\n",
    "        labels.append(f'label_{column}')\n",
    "        \n",
    "    return data, encoders, labels\n",
    "\n",
    "column_to_encode = [\n",
    "                    'is_ethicity_superior_meaning',\n",
    "                   'is_ethicity_aggressor_meaning',\n",
    "                   'is_ethicity_dangerous_meaning'\n",
    "                    ]\n",
    "\n",
    "data, encoders, labels = encode_columns(data, column_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15ec2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 23:53:43.810502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 23:53:44.298088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader , Dataset\n",
    "import nlpaug.augmenter.word\n",
    "import nlpaug.augmenter.sentence\n",
    "\n",
    "# Класс датасета для имплементации аугментации (она работает плохо)\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text, targets, document_ids, tokenizer=None, augment=False, augment_probs=None):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.document_ids = document_ids\n",
    "        self.augment = augment\n",
    "        self.tokenizer = tokenizer\n",
    "#         p = 0.5\n",
    "        if self.augment:\n",
    "            p1, p2 = augment_probs\n",
    "            self.augmenters = [\n",
    "                nlpaug.augmenter.word.random.RandomWordAug(aug_p=p, action='delete', aug_max=None, stopwords=stopwords),\n",
    "                nlpaug.augmenter.word.random.RandomWordAug(aug_p=p, action='swap', aug_max=None, stopwords=stopwords),\n",
    "             ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.text[idx])\n",
    "\n",
    "        if self.augment:\n",
    "            for aug in self.augmenters:\n",
    "                text = aug.augment(text, n=1)[0]\n",
    "\n",
    "    \n",
    "        if self.tokenizer is not None:\n",
    "    \n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                text,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = 256,           # Pad & truncate all sentences.\n",
    "                                pad_to_max_length = True,\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                                padding=\"max_length\" ,\n",
    "                                truncation = True ,\n",
    "                           )\n",
    "\n",
    "            ids = encoded_dict['input_ids'][0]\n",
    "        elif self.tokenizer is None:\n",
    "\n",
    "            ids = text[idx]\n",
    "\n",
    "        targets = self.targets[idx]\n",
    "        return ids, targets, self.document_ids[idx]\n",
    "    \n",
    "stopwords = np.unique(data['eth_group_to_code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2541a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_is_ethicity_superior_meaning',\n",
       " 'label_is_ethicity_aggressor_meaning',\n",
       " 'label_is_ethicity_dangerous_meaning']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a317a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Загружаю токенизатор\n",
    "checkpoint = 'cointegrated/rubert-tiny-toxicity'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "324715ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 11\n",
    "\n",
    "# Инициализирую даталоадеры для обучения\n",
    "def get_dataloaders(X, y, document_id, is_balanced=False,\n",
    "                    augment=False, augment_probs=None, tokenizer=None, seed_value=42):\n",
    "      \n",
    "    train_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.15,\n",
    "                                                        stratify=y[:, -1], random_state=seed_value)\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(X))[train_idx], test_size=0.15,\n",
    "                                                          stratify=y[train_idx][:, -1], random_state=seed_value)\n",
    "\n",
    "    if is_balanced:    \n",
    "        ros = RandomOverSampler()\n",
    "        indices = np.arange(len(train_idx)).reshape(-1, 1)\n",
    "        indices_os, _ = ros.fit_resample(indices, np.array(y[train_idx][:, -1]))\n",
    "        train_idx = indices_os.flatten()\n",
    "    \n",
    "    train_dataset = MyDataset(X[train_idx], y[train_idx], document_id[train_idx],\n",
    "                              augment=augment, augment_probs=augment_probs, tokenizer=tokenizer)\n",
    "    val_dataset = MyDataset(X[val_idx], y[val_idx], document_id[val_idx], tokenizer=tokenizer)\n",
    "    test_dataset = MyDataset(X[test_idx], y[test_idx], document_id[test_idx], tokenizer=tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, num_workers=12)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=12)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, num_workers=12)\n",
    "        \n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "    \n",
    "# df = pd.read_csv('data/is_ethicity_superior_meaning|is_ethicity_aggressor_meaning|is_ethicity_dangerous_meaning_eth_group_lemm_text.csv')\n",
    "# data, encoders, labels = encode_columns(df, column_to_encode)\n",
    "    \n",
    "# train_loader, val_loader, test_loader = get_dataloaders(X=data['lemm_text'].values,\n",
    "#                                                         y=torch.tensor(data[labels].values),\n",
    "#                                                         document_id=torch.tensor(data['document.id'].values),\n",
    "#                                                         is_balanced=False, augment=False, tokenizer=tokenizer,\n",
    "#                                                         seed_value=1\n",
    "#                                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60bc0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn, utils, Tensor\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "# Модель с bert слоем\n",
    "class BERT(pl.LightningModule):\n",
    "    def __init__(self, input_dim, labels, head_dims):\n",
    "        super().__init__()     \n",
    "#         checkpoint = 's-nlp/russian_toxicity_classifier'\n",
    "#         checkpoint = 'cointegrated/rubert-tiny-toxicity'\n",
    "        self.bert = BertModel.from_pretrained(checkpoint, return_dict=False)\n",
    "#         self.fc1 = nn.Linear(input_dim, 256)\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(312, 256)\n",
    "        \n",
    "        self.heads = nn.ModuleList()\n",
    "        self.loss_fns = []\n",
    "        for i, label in enumerate(labels):\n",
    "            head = nn.Linear(256, head_dims[i]).cuda()\n",
    "            self.heads.append(head)\n",
    "            \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            self.loss_fns.append(loss_fn)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ELU()\n",
    "    \n",
    "        self.labels = labels\n",
    "        self.head_dims = head_dims\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "#         label1, label2 = y[:, 0], y[:, 1]\n",
    "\n",
    "        x = self.forward(x)\n",
    "        \n",
    "        losses = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            loss = self.loss_fns[i](x[i], y[:, i])\n",
    "            self.log(f'train_loss_head_{i}', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "#         label1, label2 = y[:, 0], y[:, 1]\n",
    "        \n",
    "        x = self.forward(x)\n",
    "        losses = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            loss = self.loss_fns[i](x[i], y[:, i])\n",
    "            self.log(f'val_loss_head_{i}', loss, prog_bar=False, on_step=False, on_epoch=True)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses)\n",
    "                \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        def log_f1_score(raw_preds, targets, name, num_classes):\n",
    "            preds = torch.argmax(raw_preds, dim=-1)\n",
    "            f1_acc = multiclass_f1_score(preds, targets, num_classes=num_classes, average='macro', )\n",
    "            self.log(f'f1_score_{name}', f1_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            return f1_acc\n",
    "            \n",
    "            \n",
    "        scores = []\n",
    "        for i, label in enumerate(self.labels): \n",
    "            f1_acc = log_f1_score(x[i], y[:, i], f'head_{i}', self.head_dims[i])\n",
    "            scores.append(f1_acc)\n",
    "            \n",
    "        scores = torch.tensor(scores).mean()\n",
    "        self.log(f'f1_score_mean', scores, prog_bar=False, on_step=False, on_epoch=True)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, x = self.bert(x)\n",
    "        x = self.relu(self.dropout(x))\n",
    "        x = self.relu(self.dropout(self.fc2(x)))\n",
    "        \n",
    "        return [head(x) for head in self.heads]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=2e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f90b9c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[2, 1, 1],\n",
       "         [1, 0, 2],\n",
       "         [0, 0, 2],\n",
       "         ...,\n",
       "         [1, 1, 1],\n",
       "         [1, 2, 1],\n",
       "         [1, 1, 1]]),\n",
       " 'scores': [0.5709244457705759, 0.6011517651819198, 0.5874676119423218],\n",
       " 'accuracy_scores': [0.7112630628306025,\n",
       "  0.7473874338794995,\n",
       "  0.8126693329892917]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e0941ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import warnings\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3050884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Получаю предсказания\n",
    "def get_predictions(model, test_loader):\n",
    "    \n",
    "    predictions = torch.zeros_like(test_loader.dataset.targets)\n",
    "    \n",
    "    for i, item in enumerate(test_loader.dataset):\n",
    "        x, y, document_id = item\n",
    "        preds = model(x.unsqueeze(0).cpu())\n",
    "        predictions[i] = torch.cat([one_pred.argmax(1) for one_pred in preds])\n",
    "        \n",
    "    scores = []\n",
    "    accuracy_scores = []\n",
    "    for i in range(predictions.shape[1]):\n",
    "        scores.append(f1_score(test_loader.dataset.targets[:, i], predictions[:, i], average='macro'))\n",
    "        accuracy_scores.append(accuracy_score(test_loader.dataset.targets[:, i], predictions[:, i]))\n",
    "        \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'scores': scores,\n",
    "        'accuracy_scores': accuracy_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d3552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d466670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny-toxicity were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | bert    | BertModel  | 11.8 M\n",
      "1 | fc2     | Linear     | 80.1 K\n",
      "2 | heads   | ModuleList | 771   \n",
      "3 | dropout | Dropout    | 0     \n",
      "4 | relu    | ELU        | 0     \n",
      "---------------------------------------\n",
      "11.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.460    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65edeeeda7844765be0f47f464e803e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for label in labels:\n",
    "    one_head_results = []\n",
    "    for seed_val in range(1):\n",
    "\n",
    "        train_loader, val_loader, test_loader = get_dataloaders(X=data['lemm_text'].values,\n",
    "                                                                y=torch.tensor(data[[labels[0]]].values),\n",
    "                                                                document_id=torch.tensor(data['document.id'].values),\n",
    "                                                                is_balanced=True,\n",
    "                                                                augment=False,\n",
    "                                                                tokenizer=tokenizer,\n",
    "                                                                seed_value=seed_val\n",
    "                                                               )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(monitor=\"f1_score_mean\", min_delta=0.00,\n",
    "                                            patience=2, verbose=False, mode=\"max\")\n",
    "\n",
    "        model = BERT(input_dim=0, labels=[labels[0]], head_dims=[3, 3, 3])\n",
    "        trainer = pl.Trainer(max_epochs=10, callbacks=[early_stop_callback])\n",
    "        trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        model.eval()\n",
    "    #     model.cuda()\n",
    "\n",
    "        scores = get_predictions(model, test_loader)\n",
    "\n",
    "        one_head_results.append(scores)\n",
    "#         all_results[seed_val] = scores\n",
    "\n",
    "#     clear_output(wait=True)\n",
    "    all_results.append(one_head_results)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9542f4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[2],\n",
       "         [2],\n",
       "         [1],\n",
       "         ...,\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]),\n",
       " 'scores': [0.6072354470732253],\n",
       " 'accuracy_scores': [0.729325248355051]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8cb96b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 0.74\n",
      "0.62 0.75\n",
      "0.59 0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for head in range(3):\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    for i in range(1):\n",
    "        f1_scores.append(all_results[head][i]['scores'][0])\n",
    "        accuracy_scores.append(all_results[head][i]['accuracy_scores'][0])\n",
    "        \n",
    "    f1_scores = np.round(f1_scores, 2).mean()\n",
    "    accuracy_scores = np.round(accuracy_scores, 2).mean()\n",
    "    print(f1_scores, accuracy_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3be9c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_is_ethicity_superior_meaning',\n",
       " 'label_is_ethicity_aggressor_meaning',\n",
       " 'label_is_ethicity_dangerous_meaning']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c34cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny-toxicity were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | bert    | BertModel  | 11.8 M\n",
      "1 | fc2     | Linear     | 80.1 K\n",
      "2 | heads   | ModuleList | 2.3 K \n",
      "3 | dropout | Dropout    | 0     \n",
      "4 | relu    | ELU        | 0     \n",
      "---------------------------------------\n",
      "11.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.466    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612b8ab2a60541cd9eb815edd6518073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a180821cfe34290aef0da0816325bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/anaconda3/envs/barsik_env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     model.cuda()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         all_results[seed_val] = scores\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     clear_output(wait=True)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(scores)\n",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset):\n\u001b[1;32m      9\u001b[0m     x, y, document_id \u001b[38;5;241m=\u001b[39m item\n\u001b[0;32m---> 10\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     predictions[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([one_pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m one_pred \u001b[38;5;129;01min\u001b[39;00m preds])\n\u001b[1;32m     13\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[55], line 84\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 84\u001b[0m     _, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x))\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)))\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1014\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:231\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    228\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    234\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/barsik_env/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for seed_val in range(1,2):\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(X=data['lemm_text'].values,\n",
    "                                                            y=torch.tensor(data[labels].values),\n",
    "                                                            document_id=torch.tensor(data['document.id'].values),\n",
    "                                                            is_balanced=False,\n",
    "                                                            augment=False,\n",
    "                                                            tokenizer=tokenizer,\n",
    "                                                            seed_value=seed_val\n",
    "                                                           )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"f1_score_mean\", min_delta=0.00,\n",
    "                                        patience=2, verbose=False, mode=\"max\")\n",
    "\n",
    "    model = BERT(input_dim=0, labels=labels, head_dims=[3, 3, 3])\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=[early_stop_callback])\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    model.eval()\n",
    "#     model.cuda()\n",
    "\n",
    "    scores = get_predictions(model, test_loader)\n",
    "#         all_results[seed_val] = scores\n",
    "\n",
    "#     clear_output(wait=True)\n",
    "    all_results.append(scores)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4aeab538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         ...,\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 2, 1]]),\n",
       " 'scores': [0.5174703745486348, 0.5502895815352725, 0.43371814235001777],\n",
       " 'accuracy_scores': [0.7033931105663785,\n",
       "  0.7297122951877177,\n",
       "  0.8086698490517352]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "270c0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52 0.68\n",
      "0.56 0.7\n",
      "0.49 0.79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for head in range(3):\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    for i in range(1):\n",
    "        f1_scores.append(all_results[head][i]['scores'][0])\n",
    "        accuracy_scores.append(all_results[head][i]['accuracy_scores'][0])\n",
    "        \n",
    "    f1_scores = np.round(f1_scores, 2).mean()\n",
    "    accuracy_scores = np.round(accuracy_scores, 2).mean()\n",
    "    print(f1_scores, accuracy_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3964638b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label_is_ethicity_dangerous_meaning'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdf2e0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         ...,\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]),\n",
       " 'scores': [0.4817457929717897],\n",
       " 'accuracy_scores': [0.7227454521997162]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2ffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f161961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8061029879211697, 0.8424454333545243, 0.9102564102564102]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a490ab66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55, 0.62, 0.46])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores = np.mean(f1_scores, axis=0)\n",
    "np.round(f1_scores, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f7d82c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81, 0.84, 0.91])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores = np.mean(accuracy_scores, axis=0)\n",
    "np.round(accuracy_scores, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "57253e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "    \n",
    "for random_seed in range(4):\n",
    "    f1_scores.append(unbalanced_results[random_seed]['scores'])\n",
    "    accuracy_scores.append(all_results[random_seed]['accuracy_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2af5b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54811702, 0.61588839, 0.4561468 ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores = np.mean(f1_scores, axis=0)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad91b490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unbalanced_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1eef3704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.30      0.20       186\n",
      "           1       0.92      0.84      0.88      3771\n",
      "           2       0.28      0.40      0.33       348\n",
      "\n",
      "    accuracy                           0.78      4305\n",
      "   macro avg       0.45      0.51      0.47      4305\n",
      "weighted avg       0.84      0.78      0.81      4305\n",
      "\n",
      "[[  55   98   33]\n",
      " [ 272 3180  319]\n",
      " [  46  164  138]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "k = 2\n",
    "print(metrics.classification_report(test_loader.dataset.targets[:, k], predictions[:, k]))\n",
    "print(metrics.confusion_matrix(test_loader.dataset.targets[:, k], predictions[:, k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bd01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
